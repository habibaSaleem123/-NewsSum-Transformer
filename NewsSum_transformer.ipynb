{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEiu0PdmkQT1"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl  --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVJoWfnrkaMR",
        "outputId": "9909235c-f0ff-4f26-e0bc-a175a5d9b4fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy matplotlib seaborn pandas tensorflow scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKJqr0DtkShK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYJmGZy8kfj7"
      },
      "outputs": [],
      "source": [
        "ENCODER_LEN = 100\n",
        "DECODER_LEN = 20\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = BATCH_SIZE*8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD8Mkk7MmLuA",
        "outputId": "eaa3d852-53c4-4794-f31e-bbf0fc6019c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "KtsDSzTokhzD",
        "outputId": "1b9cbebb-2dae-4f09-aae2-3b2f3d53dda5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            Headline  \\\n",
            "0  4 ex-bank officials booked for cheating bank o...   \n",
            "1     Supreme Court to go paperless in 6 months: CJI   \n",
            "2  At least 3 killed, 30 injured in blast in Sylh...   \n",
            "3  Why has Reliance been barred from trading in f...   \n",
            "4  Was stopped from entering my own studio at Tim...   \n",
            "\n",
            "                                               Short                 Source   \\\n",
            "0  The CBI on Saturday booked four former officia...  The New Indian Express   \n",
            "1  Chief Justice JS Khehar has said the Supreme C...                 Outlook   \n",
            "2  At least three people were killed, including a...         Hindustan Times   \n",
            "3  Mukesh Ambani-led Reliance Industries (RIL) wa...                Livemint   \n",
            "4  TV news anchor Arnab Goswami has said he was t...                 YouTube   \n",
            "\n",
            "      Time  Publish Date  \n",
            "0  09:25:00   2017-03-26  \n",
            "1  22:18:00   2017-03-25  \n",
            "2  23:39:00   2017-03-25  \n",
            "3  23:08:00   2017-03-25  \n",
            "4  23:24:00   2017-03-25  \n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"news\",\n  \"rows\": 55104,\n  \"fields\": [\n    {\n      \"column\": \"Headline\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 54940,\n        \"samples\": [\n          \"Happy to be back with Salman for Tiger Zinda Hai: Kaif\",\n          \" Lawsuit claims Oculus Co-founder spread fake origin story\",\n          \"Gambhir only Indian to make 100s in 5 straight Tests\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Short\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 54997,\n        \"samples\": [\n          \"An Australian couple who own a tropical island resort in Micronesia has decided to give it away in lottery rather than sell it to the highest bidder. The lottery kicked off in April, and so far 55,500 people from 150 countries have bought tickets which start from $49. The lottery for the 16-bed resort will be drawn on Tuesday.\",\n          \"Over 200 rail accidents out of 292, from 2012-13 to January 2016, were caused due to the failure of railway staff as per the data of the Railway Ministry. The data suggested that action was being taken against 542 employees for their negligence and penalties were imposed in over 500 cases based on investigations by Commission of Railway Safety (CRS).\",\n          \"A 1400-km long optical fibre cable has been built connecting the two most precise optical atomic clocks in Europe, located in Germany and France. This marks the first and most accurate comparison of atomic clocks across national borders, a feat that may allow time-sensitive scientific experiments like observing changes in the values of fundamental physical constants over time.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "news"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-776f8990-f02b-499c-8d47-584d82ca8d56\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-776f8990-f02b-499c-8d47-584d82ca8d56')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-776f8990-f02b-499c-8d47-584d82ca8d56 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-776f8990-f02b-499c-8d47-584d82ca8d56');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f6cbf4e3-aaa8-4f8a-814d-70a955ec6d68\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f6cbf4e3-aaa8-4f8a-814d-70a955ec6d68')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f6cbf4e3-aaa8-4f8a-814d-70a955ec6d68 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                            Headline  \\\n",
              "0  4 ex-bank officials booked for cheating bank o...   \n",
              "1     Supreme Court to go paperless in 6 months: CJI   \n",
              "2  At least 3 killed, 30 injured in blast in Sylh...   \n",
              "3  Why has Reliance been barred from trading in f...   \n",
              "4  Was stopped from entering my own studio at Tim...   \n",
              "\n",
              "                                               Short  \n",
              "0  The CBI on Saturday booked four former officia...  \n",
              "1  Chief Justice JS Khehar has said the Supreme C...  \n",
              "2  At least three people were killed, including a...  \n",
              "3  Mukesh Ambani-led Reliance Industries (RIL) wa...  \n",
              "4  TV news anchor Arnab Goswami has said he was t...  "
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to the file in Google Drive\n",
        "file_path = '/content/drive/My Drive/Inshorts Cleaned Data.xlsx'\n",
        "\n",
        "# Load the Excel file into a DataFrame\n",
        "news = pd.read_excel(file_path, engine='openpyxl')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(news.head())\n",
        "\n",
        "news.drop(['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True)\n",
        "news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2_emp_6nU1G"
      },
      "outputs": [],
      "source": [
        "article = news['Short']\n",
        "summary = news['Headline']\n",
        "article = article.apply(lambda x: '<SOS> ' + x + ' <EOS>')\n",
        "summary = summary.apply(lambda x: '<SOS> ' + x + ' <EOS>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM2oHA89nXfX"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    text = re.sub(r\"&.[1-9]+;\",\" \",text)\n",
        "    return text\n",
        "article = article.apply(lambda x: preprocess(x))\n",
        "summary = summary.apply(lambda x: preprocess(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi1Miqo2nZUa"
      },
      "outputs": [],
      "source": [
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'\n",
        "article_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n",
        "article_tokenizer.fit_on_texts(article)\n",
        "summary_tokenizer.fit_on_texts(summary)\n",
        "inputs = article_tokenizer.texts_to_sequences(article)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SapXVD27nc-c",
        "outputId": "f9335218-199f-4567-e679-a38b58215558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76362 29661\n"
          ]
        }
      ],
      "source": [
        "ENCODER_VOCAB = len(article_tokenizer.word_index) + 1\n",
        "DECODER_VOCAB = len(summary_tokenizer.word_index) + 1\n",
        "print(ENCODER_VOCAB, DECODER_VOCAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBzbhBQfneyh"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')\n",
        "inputs = tf.cast(inputs, dtype=tf.int64)\n",
        "targets = tf.cast(targets, dtype=tf.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-DBebSmng8j"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smZQ8RMKnhwW"
      },
      "outputs": [],
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVjuwI5YnkZt"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mo1w0aDnnwR"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t21iVL2VnqVS"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc3FqOsyntIN"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "\n",
        "        return x\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        return x, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U76TsZ7Fnvsk"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training=False, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n",
        "    # Ensure all arguments are handled and are passed as keyword arguments\n",
        "        # Encoder pass\n",
        "\n",
        "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
        "\n",
        "        # Decoder pass\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask\n",
        "        )\n",
        "\n",
        "        # Final output\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtECvNuCoVmi"
      },
      "outputs": [],
      "source": [
        "num_layers = 3\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 4\n",
        "dropout_rate = 0.2\n",
        "EPOCHS = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SOT88lTnyOV"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)  # Cast step to float32\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJqCkk6ln0g9"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model=512, warmup_steps=8000)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM8en5IzobeJ"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "    #accuracies = tf.cast(accuracies, dtype= tf.float32)\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiSDKbUqoeN8"
      },
      "outputs": [],
      "source": [
        "# Initialize loss and accuracy metrics\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        " # or SparseCategoricalAccuracy depending on your labels\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "# Optionally, you can initialize other metrics like 'train_accuracy' if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLdLUebyogGJ"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=ENCODER_VOCAB,\n",
        "    target_vocab_size=DECODER_VOCAB,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOLcgHymohk6"
      },
      "outputs": [],
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGk8umKQokDY"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t15zXIgol05"
      },
      "outputs": [],
      "source": [
        "# Re-create the optimizer after any model changes\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp,\n",
        "            training=True,  # Specify explicitly as a keyword argument\n",
        "            enc_padding_mask=enc_padding_mask,\n",
        "            look_ahead_mask=combined_mask,\n",
        "            dec_padding_mask=dec_padding_mask\n",
        "        )\n",
        "        # Using SparseCategoricalCrossentropy loss function\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "\n",
        "    # Convert predictions to token indices\n",
        "    predicted_ids = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "    # Update accuracy using predicted token indices\n",
        "    train_accuracy.update_state(tar_real, predicted_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH4yQD-pCxlb",
        "outputId": "72fcc7d9-d32b-49e0-d41a-e01aff2b28ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 10.2943 Accuracy 0.0703\n",
            "Epoch 1 Batch 100 Loss 9.2053 Accuracy 0.0954\n",
            "Epoch 1 Batch 200 Loss 8.4385 Accuracy 0.0997\n",
            "Epoch 1 Batch 300 Loss 7.7578 Accuracy 0.0988\n",
            "Epoch 1 Batch 400 Loss 7.1570 Accuracy 0.1017\n",
            "Epoch 1 Batch 500 Loss 6.6251 Accuracy 0.1046\n",
            "Epoch 1 Batch 600 Loss 6.1779 Accuracy 0.1101\n",
            "Epoch 1 Batch 700 Loss 5.8222 Accuracy 0.1150\n",
            "Epoch 1 Batch 800 Loss 5.5389 Accuracy 0.1186\n",
            "Epoch 1 Loss 5.3951 Accuracy 0.1212\n",
            "Time taken for 1 epoch: 112.67685961723328 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 5.3940 Accuracy 0.1211\n",
            "Epoch 2 Batch 100 Loss 5.2744 Accuracy 0.1192\n",
            "Epoch 2 Batch 200 Loss 5.1679 Accuracy 0.1176\n",
            "Epoch 2 Batch 300 Loss 5.0700 Accuracy 0.1160\n",
            "Epoch 2 Batch 400 Loss 4.9764 Accuracy 0.1150\n",
            "Epoch 2 Batch 500 Loss 4.8795 Accuracy 0.1159\n",
            "Epoch 2 Batch 600 Loss 4.7817 Accuracy 0.1174\n",
            "Epoch 2 Batch 700 Loss 4.6897 Accuracy 0.1192\n",
            "Epoch 2 Batch 800 Loss 4.6059 Accuracy 0.1208\n",
            "Epoch 2 Loss 4.5588 Accuracy 0.1221\n",
            "Time taken for 1 epoch: 90.10141491889954 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.5586 Accuracy 0.1221\n",
            "Epoch 3 Batch 100 Loss 4.5301 Accuracy 0.1215\n",
            "Epoch 3 Batch 200 Loss 4.5007 Accuracy 0.1218\n",
            "Epoch 3 Batch 300 Loss 4.4689 Accuracy 0.1228\n",
            "Epoch 3 Batch 400 Loss 4.4342 Accuracy 0.1236\n",
            "Epoch 3 Batch 500 Loss 4.3927 Accuracy 0.1246\n",
            "Epoch 3 Batch 600 Loss 4.3462 Accuracy 0.1256\n",
            "Epoch 3 Batch 700 Loss 4.2997 Accuracy 0.1267\n",
            "Epoch 3 Batch 800 Loss 4.2547 Accuracy 0.1282\n",
            "Epoch 3 Loss 4.2283 Accuracy 0.1289\n",
            "Time taken for 1 epoch: 90.14744091033936 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 4.2282 Accuracy 0.1289\n",
            "Epoch 4 Batch 100 Loss 4.2156 Accuracy 0.1288\n",
            "Epoch 4 Batch 200 Loss 4.2013 Accuracy 0.1284\n",
            "Epoch 4 Batch 300 Loss 4.1843 Accuracy 0.1279\n",
            "Epoch 4 Batch 400 Loss 4.1644 Accuracy 0.1276\n",
            "Epoch 4 Batch 500 Loss 4.1387 Accuracy 0.1277\n",
            "Epoch 4 Batch 600 Loss 4.1086 Accuracy 0.1283\n",
            "Epoch 4 Batch 700 Loss 4.0771 Accuracy 0.1291\n",
            "Epoch 4 Batch 800 Loss 4.0459 Accuracy 0.1300\n",
            "Epoch 4 Loss 4.0272 Accuracy 0.1305\n",
            "Time taken for 1 epoch: 90.30228543281555 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 4.0272 Accuracy 0.1305\n",
            "Epoch 5 Batch 100 Loss 4.0193 Accuracy 0.1301\n",
            "Epoch 5 Batch 200 Loss 4.0096 Accuracy 0.1298\n",
            "Epoch 5 Batch 300 Loss 3.9976 Accuracy 0.1295\n",
            "Epoch 5 Batch 400 Loss 3.9833 Accuracy 0.1294\n",
            "Epoch 5 Batch 500 Loss 3.9642 Accuracy 0.1298\n",
            "Epoch 5 Batch 600 Loss 3.9416 Accuracy 0.1304\n",
            "Epoch 5 Batch 700 Loss 3.9175 Accuracy 0.1313\n",
            "Epoch 5 Batch 800 Loss 3.8931 Accuracy 0.1320\n",
            "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
            "Epoch 5 Loss 3.8783 Accuracy 0.1324\n",
            "Time taken for 1 epoch: 90.89781761169434 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.8783 Accuracy 0.1324\n",
            "Epoch 6 Batch 100 Loss 3.8721 Accuracy 0.1323\n",
            "Epoch 6 Batch 200 Loss 3.8642 Accuracy 0.1322\n",
            "Epoch 6 Batch 300 Loss 3.8545 Accuracy 0.1320\n",
            "Epoch 6 Batch 400 Loss 3.8428 Accuracy 0.1321\n",
            "Epoch 6 Batch 500 Loss 3.8273 Accuracy 0.1324\n",
            "Epoch 6 Batch 600 Loss 3.8088 Accuracy 0.1328\n",
            "Epoch 6 Batch 700 Loss 3.7890 Accuracy 0.1333\n",
            "Epoch 6 Batch 800 Loss 3.7687 Accuracy 0.1337\n",
            "Epoch 6 Loss 3.7564 Accuracy 0.1342\n",
            "Time taken for 1 epoch: 90.232670545578 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 3.7564 Accuracy 0.1342\n",
            "Epoch 7 Batch 100 Loss 3.7509 Accuracy 0.1341\n",
            "Epoch 7 Batch 200 Loss 3.7439 Accuracy 0.1341\n",
            "Epoch 7 Batch 300 Loss 3.7354 Accuracy 0.1342\n",
            "Epoch 7 Batch 400 Loss 3.7253 Accuracy 0.1343\n",
            "Epoch 7 Batch 500 Loss 3.7120 Accuracy 0.1345\n",
            "Epoch 7 Batch 600 Loss 3.6962 Accuracy 0.1348\n",
            "Epoch 7 Batch 700 Loss 3.6793 Accuracy 0.1353\n",
            "Epoch 7 Batch 800 Loss 3.6618 Accuracy 0.1357\n",
            "Epoch 7 Loss 3.6512 Accuracy 0.1360\n",
            "Time taken for 1 epoch: 90.15058970451355 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 3.6511 Accuracy 0.1360\n",
            "Epoch 8 Batch 100 Loss 3.6460 Accuracy 0.1360\n",
            "Epoch 8 Batch 200 Loss 3.6398 Accuracy 0.1360\n",
            "Epoch 8 Batch 300 Loss 3.6321 Accuracy 0.1361\n",
            "Epoch 8 Batch 400 Loss 3.6231 Accuracy 0.1362\n",
            "Epoch 8 Batch 500 Loss 3.6115 Accuracy 0.1366\n",
            "Epoch 8 Batch 600 Loss 3.5977 Accuracy 0.1369\n",
            "Epoch 8 Batch 700 Loss 3.5828 Accuracy 0.1374\n",
            "Epoch 8 Batch 800 Loss 3.5675 Accuracy 0.1378\n",
            "Epoch 8 Loss 3.5582 Accuracy 0.1381\n",
            "Time taken for 1 epoch: 141.91206622123718 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 3.5582 Accuracy 0.1381\n",
            "Epoch 9 Batch 100 Loss 3.5534 Accuracy 0.1381\n",
            "Epoch 9 Batch 200 Loss 3.5477 Accuracy 0.1380\n",
            "Epoch 9 Batch 300 Loss 3.5408 Accuracy 0.1381\n",
            "Epoch 9 Batch 400 Loss 3.5326 Accuracy 0.1383\n",
            "Epoch 9 Batch 500 Loss 3.5222 Accuracy 0.1386\n",
            "Epoch 9 Batch 600 Loss 3.5100 Accuracy 0.1389\n",
            "Epoch 9 Batch 700 Loss 3.4968 Accuracy 0.1393\n",
            "Epoch 9 Batch 800 Loss 3.4831 Accuracy 0.1398\n",
            "Epoch 9 Loss 3.4748 Accuracy 0.1401\n",
            "Time taken for 1 epoch: 90.29610466957092 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 3.4747 Accuracy 0.1401\n",
            "Epoch 10 Batch 100 Loss 3.4703 Accuracy 0.1401\n",
            "Epoch 10 Batch 200 Loss 3.4649 Accuracy 0.1402\n",
            "Epoch 10 Batch 300 Loss 3.4585 Accuracy 0.1403\n",
            "Epoch 10 Batch 400 Loss 3.4510 Accuracy 0.1404\n",
            "Epoch 10 Batch 500 Loss 3.4416 Accuracy 0.1406\n",
            "Epoch 10 Batch 600 Loss 3.4304 Accuracy 0.1409\n",
            "Epoch 10 Batch 700 Loss 3.4185 Accuracy 0.1413\n",
            "Epoch 10 Batch 800 Loss 3.4061 Accuracy 0.1417\n",
            "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
            "Epoch 10 Loss 3.3985 Accuracy 0.1420\n",
            "Time taken for 1 epoch: 90.85464286804199 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 3.3985 Accuracy 0.1420\n",
            "Epoch 11 Batch 100 Loss 3.3943 Accuracy 0.1420\n",
            "Epoch 11 Batch 200 Loss 3.3892 Accuracy 0.1421\n",
            "Epoch 11 Batch 300 Loss 3.3832 Accuracy 0.1423\n",
            "Epoch 11 Batch 400 Loss 3.3764 Accuracy 0.1425\n",
            "Epoch 11 Batch 500 Loss 3.3676 Accuracy 0.1428\n",
            "Epoch 11 Batch 600 Loss 3.3574 Accuracy 0.1431\n",
            "Epoch 11 Batch 700 Loss 3.3465 Accuracy 0.1436\n",
            "Epoch 11 Batch 800 Loss 3.3351 Accuracy 0.1440\n",
            "Epoch 11 Loss 3.3282 Accuracy 0.1443\n",
            "Time taken for 1 epoch: 90.34432291984558 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 3.3282 Accuracy 0.1443\n",
            "Epoch 12 Batch 100 Loss 3.3241 Accuracy 0.1444\n",
            "Epoch 12 Batch 200 Loss 3.3193 Accuracy 0.1444\n",
            "Epoch 12 Batch 300 Loss 3.3136 Accuracy 0.1445\n",
            "Epoch 12 Batch 400 Loss 3.3071 Accuracy 0.1447\n",
            "Epoch 12 Batch 500 Loss 3.2991 Accuracy 0.1449\n",
            "Epoch 12 Batch 600 Loss 3.2896 Accuracy 0.1452\n",
            "Epoch 12 Batch 700 Loss 3.2795 Accuracy 0.1457\n",
            "Epoch 12 Batch 800 Loss 3.2691 Accuracy 0.1461\n",
            "Epoch 12 Loss 3.2626 Accuracy 0.1464\n",
            "Time taken for 1 epoch: 90.37874412536621 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 3.2626 Accuracy 0.1464\n",
            "Epoch 13 Batch 100 Loss 3.2586 Accuracy 0.1464\n",
            "Epoch 13 Batch 200 Loss 3.2540 Accuracy 0.1465\n",
            "Epoch 13 Batch 300 Loss 3.2486 Accuracy 0.1466\n",
            "Epoch 13 Batch 400 Loss 3.2424 Accuracy 0.1469\n",
            "Epoch 13 Batch 500 Loss 3.2348 Accuracy 0.1472\n",
            "Epoch 13 Batch 600 Loss 3.2260 Accuracy 0.1475\n",
            "Epoch 13 Batch 700 Loss 3.2166 Accuracy 0.1479\n",
            "Epoch 13 Batch 800 Loss 3.2068 Accuracy 0.1482\n",
            "Epoch 13 Loss 3.2009 Accuracy 0.1485\n",
            "Time taken for 1 epoch: 90.3278501033783 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 3.2008 Accuracy 0.1485\n",
            "Epoch 14 Batch 100 Loss 3.1970 Accuracy 0.1485\n",
            "Epoch 14 Batch 200 Loss 3.1924 Accuracy 0.1487\n",
            "Epoch 14 Batch 300 Loss 3.1872 Accuracy 0.1488\n",
            "Epoch 14 Batch 400 Loss 3.1814 Accuracy 0.1489\n",
            "Epoch 14 Batch 500 Loss 3.1742 Accuracy 0.1492\n",
            "Epoch 14 Batch 600 Loss 3.1659 Accuracy 0.1495\n",
            "Epoch 14 Batch 700 Loss 3.1571 Accuracy 0.1499\n",
            "Epoch 14 Batch 800 Loss 3.1479 Accuracy 0.1504\n",
            "Epoch 14 Loss 3.1423 Accuracy 0.1506\n",
            "Time taken for 1 epoch: 90.17097282409668 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 3.1423 Accuracy 0.1506\n",
            "Epoch 15 Batch 100 Loss 3.1385 Accuracy 0.1506\n",
            "Epoch 15 Batch 200 Loss 3.1340 Accuracy 0.1508\n",
            "Epoch 15 Batch 300 Loss 3.1290 Accuracy 0.1509\n",
            "Epoch 15 Batch 400 Loss 3.1234 Accuracy 0.1511\n",
            "Epoch 15 Batch 500 Loss 3.1166 Accuracy 0.1514\n",
            "Epoch 15 Batch 600 Loss 3.1087 Accuracy 0.1517\n",
            "Epoch 15 Batch 700 Loss 3.1004 Accuracy 0.1522\n",
            "Epoch 15 Batch 800 Loss 3.0917 Accuracy 0.1525\n",
            "Saving checkpoint for epoch 15 at checkpoints/ckpt-3\n",
            "Epoch 15 Loss 3.0864 Accuracy 0.1528\n",
            "Time taken for 1 epoch: 91.26174688339233 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "        # Reset metrics manually by initializing accumulators\n",
        "    epoch_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPMURgr94q-9"
      },
      "outputs": [],
      "source": [
        "def evaluate(input_article):\n",
        "    input_article = article_tokenizer.texts_to_sequences([input_article])\n",
        "    input_article = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        input_article, maxlen=ENCODER_LEN, padding='post', truncating='post'\n",
        "    )\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_article[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index['<sos>']]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    for i in range(DECODER_LEN):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        # Debug combined_mask dimensions\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            inp=encoder_input,\n",
        "            tar=output,\n",
        "            training=False,\n",
        "            enc_padding_mask=enc_padding_mask,\n",
        "            look_ahead_mask=combined_mask,\n",
        "            dec_padding_mask=dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        predictions = predictions[:, -1:, :]  # Get the last prediction\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index['<eos>']:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr19IPN257m-"
      },
      "outputs": [],
      "source": [
        "def summarize(input_article):\n",
        "    summarized, _ = evaluate(input_article=input_article)\n",
        "    summarized = summarized.numpy()\n",
        "\n",
        "    # Remove the <sos> token\n",
        "    summarized = np.expand_dims(summarized[1:], 0)\n",
        "\n",
        "    # Convert tokens to text\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "0txUWg-07NIH",
        "outputId": "9cc06b63-00ed-4e75-c889-6ea83aa4b928"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<SOS> Uttar Pradesh Chief Minister Yogi Adityanath, during his visit to Gorakhpur on Saturday, said the anti-Romeo squads must not trouble consenting boys and girls unnecessarily. This comes after three cops were suspended in Ghaziabad for wrongfully detaining a couple in the  anti-Romeo  drive. However, he asserted that stern actions would be taken against the eve-teasers. <EOS>'"
            ]
          },
          "execution_count": 273,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "article[7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbNfxUnNKQ7e"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMZrV39NBIfI",
        "outputId": "0d9d30dd-97e0-4bcc-e3ea-672b41923b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real Headline :  nti-Romeo squads must not trouble consenting youths: Yog \n",
            "Predicted Summary :  no bharat mata and bharat mata ki jai\n"
          ]
        }
      ],
      "source": [
        "print(\"Real Headline : \", summary[7][7:-7],\"\\nPredicted Summary : \", summarize(article[7]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2_-kmJ6KcSP"
      },
      "outputs": [],
      "source": [
        "def evaluate(input_article):\n",
        "    input_article = article_tokenizer.texts_to_sequences([input_article])\n",
        "    input_article = tf.keras.preprocessing.sequence.pad_sequences(input_article, maxlen=ENCODER_LEN,\n",
        "                                                                   padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_article[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index['<sos>']]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    for i in range(DECODER_LEN):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index['<eos>']:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0FAf2ytKdX7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def summarize(input_article):\n",
        "    summarized = evaluate(input_article=input_article)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Gy2lNcIRKgKf",
        "outputId": "90a0a0b2-b0e1-4526-f814-fe294254e94f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<SOS> A new trailer for the upcoming superhero film  Justice League  was released on Saturday. Based on the DC Comics superhero team, the film stars Ben Affleck as  Batman , Gal Gadot as  Wonder Woman , Ezra Miller as  The Flash  and Jason Momoa as  Aquaman . Directed by Zack Snyder, the film is scheduled to release on November 17, 2017. <EOS>'"
            ]
          },
          "execution_count": 282,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "article[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddqmRNskKkt-",
        "outputId": "1ca30fea-4a32-4272-bc6f-b5c84f2aa5a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real Headline :   New trailer of  Justice League  released  \n",
            " Predicted Summary :  new trailer of fashion week released\n"
          ]
        }
      ],
      "source": [
        "print(\"Real Headline : \", summary[5][5:-5],\"\\n Predicted Summary : \", summarize(article[5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzSNFV27KvSy",
        "outputId": "ed1c830e-d19b-4721-f8e0-a81a17063a2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real Headline :   BSF gets first woman field officer after 51 years  \n",
            "Predicted Summary :  two woman becomes first woman in assam\n"
          ]
        }
      ],
      "source": [
        "print(\"Real Headline : \", summary[25][5:-5],\"\\nPredicted Summary : \", summarize(article[25]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "5-ry7bNlLIA-",
        "outputId": "22359d84-85ce-4137-e359-35ae92fa62ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<SOS> Actor Amitabh Bachchan along with 14 other artistes have sung a track named  Namami Brahmaputra , which pays tribute to the Brahmaputra river. Other singers who have sung the song include Arijit Singh, Sonu Nigam, Shreya Ghoshal and Shankar Mahadevan. The song, composed by Papon, was released as part of a mega cultural festival that has been organised in Assam. <EOS>'"
            ]
          },
          "execution_count": 287,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "article[90]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6CCKw1dLJiY",
        "outputId": "aae8d2cd-0780-4066-c056-dfe4e0668560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real Headline :   Amitabh Bachchan sings song that pays tribute to Brahmaputra  \n",
            "Predicted Summary :  amitabh bachchan features on kabali song\n"
          ]
        }
      ],
      "source": [
        "print(\"Real Headline : \", summary[90][5:-5],\"\\nPredicted Summary : \", summarize(article[90]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "br5RMbvRLTOs",
        "outputId": "cee465a7-817d-42e3-db41-6e1a388c2b19"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<SOS> Indian spinner Ravichandran Ashwin has broken the record for most wickets in a single Test season, taking his 79th this season during the Dharamsala Test against Australia on Saturday. Ashwin went past South African pacer Dale Steyn, who had claimed 78 wickets in 12 Tests in 2007-08. Ashwin has taken seven five-wicket hauls this season in 13 matches. <EOS>'"
            ]
          },
          "execution_count": 289,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "article[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmGyaW-bLWHh",
        "outputId": "863085d5-b9f2-44db-e8fc-44f62df36d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real Headline :   Ashwin breaks record for most wickets in a Test season  \n",
            "Predicted Summary :  ashwin becomes most test test series on this day\n"
          ]
        }
      ],
      "source": [
        "print(\"Real Headline : \", summary[100][5:-5],\"\\nPredicted Summary : \", summarize(article[100]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}